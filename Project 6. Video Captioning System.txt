
A video captioning system generates textual descriptions for videos by analyzing both visual content (frames) and audio content (speech). In this project, we simulate a simple video captioning system that generates captions for a given video.

We’ll combine frame extraction (using OpenCV) and audio transcription (using a speech-to-text model) to generate basic captions for the video.

Here’s the Python implementation:

import cv2
import numpy as np
from transformers import BlipProcessor, BlipForConditionalGeneration
import deepspeech
from scipy.io import wavfile

# Load pre-trained BLIP model for image captioning
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
caption_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Load DeepSpeech model for audio transcription
speech_model = deepspeech.Model("deepspeech-0.9.3-models.pbmm")

# Function to generate image captions using BLIP
def generate_caption(image):
    inputs = processor(images=image, return_tensors="pt")
    out = caption_model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)

# Function to transcribe audio using DeepSpeech
def transcribe_audio(audio_file):
    fs, audio = wavfile.read(audio_file)
    audio_input = np.array(audio, dtype=np.float32)
    return speech_model.stt(audio_input)

# Video file path and audio file path
video_file = "example_video.mp4"
audio_file = "example_audio.wav"

# Step 1: Extract frames from the video and generate captions for them
cap = cv2.VideoCapture(video_file)
frame_rate = cap.get(cv2.CAP_PROP_FPS)
frame_count = 0

# Step 2: Transcribe audio for captions
audio_transcription = transcribe_audio(audio_file)
print(f"Audio Transcription: {audio_transcription}")

# Step 3: Process video frames to generate captions
while True:
    success, frame = cap.read()
    if not success:
        break

    # Generate caption for each frame
    caption = generate_caption(frame)
    print(f"Frame {frame_count} Caption: {caption}")

    frame_count += 1

# Release the video capture object
cap.release()

# Example of combining audio and visual captions into final output
final_caption = f"Video Caption: {audio_transcription}. Additional frame captions generated."
print(f"Final Caption: {final_caption}")
What This Does:
Audio: We transcribe speech from the video's audio using DeepSpeech.

Video: We process each frame of the video using the BLIP model for image captioning.

Final Captioning: We combine the audio transcription with the frame captions for a complete video description.
